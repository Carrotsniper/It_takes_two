{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "EPS = torch.finfo(torch.float).eps # Numerical logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VRNN(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim, z_dim, n_layers, bias = False):\n",
    "        super(VRNN, self).__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim = h_dim\n",
    "        self.z_dim = z_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Feature extraction transformations (对应原文公式(7) 和 (9))\n",
    "        # 这里首先需要将输入的x_data 进行embedding\n",
    "        self.phi_x = nn.Sequential(\n",
    "                    nn.Linear(x_dim, h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(h_dim, h_dim),\n",
    "                    nn.ReLU())\n",
    "        \n",
    "        # 这里对应原文公式的 (6) 和 (7), 将z进行embedding\n",
    "        self.phi_z = nn.Sequential(\n",
    "                    nn.Linear(z_dim, h_dim),\n",
    "                    nn.ReLU())\n",
    "            \n",
    "        \n",
    "        # Encoder 这里对隐藏层的均值和方差进行提取\n",
    "        # 对应原文的公式(9), 第一个 h_dim 对应 x_data 数据, 第二个 h_dim 对应的是 h_[t-1]\n",
    "        self.enc = nn.Sequential(\n",
    "                    nn.Linear(h_dim + h_dim, h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(h_dim, h_dim),\n",
    "                    nn.ReLU())\n",
    "        \n",
    "        # 对应公式 (9), 从encoder中获取 均值 和 方差\n",
    "        self.enc_mean = nn.Linear(h_dim, z_dim)\n",
    "        self.enc_std = nn.Sequential(\n",
    "                    nn.Linear(h_dim, z_dim),\n",
    "                    nn.Softplus())\n",
    "        \n",
    "        # Prior 对先验进行定义并且抽取先验的均值和方差\n",
    "        # 这里对应了公式(5), h_dim 对应的是 h_[t-1], 获取先验的 均值 和 方差\n",
    "        self.prior = nn.Sequential(\n",
    "                    nn.Linear(h_dim, h_dim),\n",
    "                    nn.ReLU())\n",
    "        self.prior_mean = nn.Linear(h_dim, z_dim)\n",
    "        self.prior_std = nn.Sequential(\n",
    "                    nn.Linear(h_dim, z_dim),\n",
    "                    nn.Softplus())\n",
    "        \n",
    "        # Decoder, 对应原文公式(6)\n",
    "        # 第一个 h_dim 对应的是 Prior 的embedding, 第二个 h_dim 对应的是 h_[t-1]\n",
    "        self.dec = nn.Sequential(\n",
    "                    nn.Linear(h_dim + h_dim, h_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(h_dim, h_dim),\n",
    "                    nn.ReLU())\n",
    "        # 获取decoder 的 均值 和 方差\n",
    "        self.dec_std = nn.Sequential(\n",
    "                    nn.Linear(h_dim, x_dim),\n",
    "                    nn.Softplus())\n",
    "        self.dec_mean = nn.Sequential(\n",
    "                    nn.Linear(h_dim, x_dim),\n",
    "                    nn.Sigmoid())\n",
    "        \n",
    "        # Recurrence, 对应原文公式(7), 使用 RNN （GRU）来封装时间序列特征\n",
    "        # 第一个 h_dim 对应的是 x_data 的embedding, 第二个 h_dim 对应的是 z 的embedding, \n",
    "        # 第三个 h_dim 对应的是 h_[t-1]\n",
    "        self.rnn = nn.GRU(h_dim + h_dim, h_dim, n_layers, bias)\n",
    "    \n",
    "    # x: torch.Size([28, 8, 28]) ([时间步，batch，特征])\n",
    "    # 简单来说，时间步就是咱们公式里面的 timestep t\n",
    "    def forward(self, x):\n",
    "        all_enc_mean, all_enc_std = [], []\n",
    "        all_dec_mean, all_dec_std = [], []\n",
    "        kld_loss = 0\n",
    "        nll_loss = 0\n",
    "        \n",
    "        # 初始化hidden state， RNN的层数， batch的大小， 隐藏层的维度\n",
    "        h = torch.zeros(self.n_layers, x.size(1), self.h_dim, device=device)\n",
    "        \n",
    "        # 输入数据的格式是(num_data, feature_dim)\n",
    "        # 遍历每一个time_step\n",
    "        for t in range(x.size(0)):\n",
    "            \n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "\n",
    "            # 对应原文公式 (7) 的第一部分\n",
    "            # 这里先对输入的数据x在每一个 time step 进行线性层包装\n",
    "            phi_x_t = self.phi_x(x[t])\n",
    "            \n",
    "            # 调用 Encoder 部分\n",
    "            # 对应公式(9)，phi_x_t (第一部分); h[-1] (对应的是h 的第三个维度，也就是 h_dim)\n",
    "            # 并且得到均值和方差\n",
    "            enc_t = self.enc(torch.cat([phi_x_t, h[-1]], 1))\n",
    "            enc_mean_t = self.enc_mean(enc_t)\n",
    "            enc_std_t = self.enc_std(enc_t)\n",
    "\n",
    "            # Sampling and reparameterization\n",
    "            # 从encoder 中的方差和均值 使用 重参数化技巧进行采样， 这里面的 z 是从encoder 的数据中采样得到的\n",
    "            z_t = self._reparameterized_sample(enc_mean_t, enc_std_t)\n",
    "            # 对应公式 (6) 的第一部分\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "\n",
    "            # Prior\n",
    "            # 对应公式(5), 得到先验标准Gaussian Distribution 的均值和方差\n",
    "            # (对应的是h 的第三个维度，也就是 h_dim)，这里面的 z 是从上一个 hidden state 中 encode 出来的\n",
    "            # ^_^ 现在出现了两个 z 哦，一个是采样来的，一个是从隐状态得到的~s\n",
    "            prior_t = self.prior(h[-1])\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "            \n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            # Decoder\n",
    "            # 对应公式 (6) 的第二部分, 从 z 和 h 中共同采样的均值和方差\n",
    "            # 去重建 x_t, 重建的时候 我们用 Encoder 采样得来的 z 和 hidden state 共同去重建 x\n",
    "            # h[-1] (对应的是h 的第三个维度，也就是 h_dim)\n",
    "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
    "            dec_mean_t = self.dec_mean(dec_t)\n",
    "            dec_std_t = self.dec_std(dec_t)\n",
    "\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            # ----------------------------------------------------------------------------------- #\n",
    "            \n",
    "            # Recurrence\n",
    "            # 这里开始使用 Recurrent Neural Network 来更新中间隐藏层\n",
    "            # 公式 (7)\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t],1).unsqueeze(0), h)\n",
    "\n",
    "            \n",
    "            # Computing the loss\n",
    "            # KL 散度计算的是 Encoder 采样出来的 z 和 先验的 z （也就是从h里面得到的z）\n",
    "            kld_loss += self._kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n",
    "            #nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\n",
    "            nll_loss += self._nll_bernoulli(dec_mean_t, x[t])\n",
    "\n",
    "            all_enc_std.append(enc_std_t)\n",
    "            all_enc_mean.append(enc_mean_t)\n",
    "            all_dec_mean.append(dec_mean_t)\n",
    "            all_dec_std.append(dec_std_t)\n",
    "            \n",
    "        return kld_loss, nll_loss, \\\n",
    "            (all_enc_mean, all_enc_std), \\\n",
    "            (all_dec_mean, all_dec_std)\n",
    "    \n",
    "    def sample(self, seq_len):\n",
    "        sample = torch.zeros(seq_len, self.x_dim, device=device)\n",
    "        \n",
    "        h = torch.zeros(self.n_layers, 1, self.h_dim, device=device)\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "            # prior\n",
    "            prior_t = self.prior(h[-1])\n",
    "            prior_mean_t = self.prior_mean(prior_t)\n",
    "            prior_std_t = self.prior_std(prior_t)\n",
    "            \n",
    "            # Sampling and reparameterization\n",
    "            z_t = self._reparameterized_sample(prior_mean_t, prior_std_t)\n",
    "            phi_z_t = self.phi_z(z_t)\n",
    "            \n",
    "            # decoder\n",
    "            dec_t = self.dec(torch.cat([phi_z_t, h[-1]], 1))\n",
    "            dec_mean_t = self.dec_mean(dec_t)\n",
    "            #dec_std_t = self.dec_std(dec_t)\n",
    "            \n",
    "            phi_x_t = self.phi_x(dec_mean_t)\n",
    "            \n",
    "            # recurrence\n",
    "            _, h = self.rnn(torch.cat([phi_x_t, phi_z_t], 1).unsqueeze(0), h)\n",
    "            \n",
    "            sample[t] = dec_mean_t.data\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def reset_parameters(self, stdv=1e-1):\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(0, stdv)\n",
    "\n",
    "\n",
    "    def _init_weights(self, stdv):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _reparameterized_sample(self, mean, std):\n",
    "        \"\"\"using std to sample\"\"\"\n",
    "        eps = torch.empty(size=std.size(), device=device, dtype=torch.float).normal_()\n",
    "        return eps.mul(std).add_(mean)\n",
    "    \n",
    "    \n",
    "    def _kld_gauss(self, mean_1, std_1, mean_2, std_2):\n",
    "        \"\"\"Using std to compute KLD\"\"\"\n",
    "\n",
    "        kld_element =  (2 * torch.log(std_2 + EPS) - 2 * torch.log(std_1 + EPS) + \n",
    "            (std_1.pow(2) + (mean_1 - mean_2).pow(2)) /\n",
    "            std_2.pow(2) - 1)\n",
    "        return\t0.5 * torch.sum(kld_element)\n",
    "\n",
    "\n",
    "    def _nll_bernoulli(self, theta, x):\n",
    "        return - torch.sum(x*torch.log(theta + EPS) + (1-x)*torch.log(1-theta-EPS))\n",
    "\n",
    "\n",
    "    def _nll_gauss(self, mean, std, x):\n",
    "        return torch.sum(torch.log(std + EPS) + torch.log(2*torch.pi)/2 + (x - mean).pow(2)/(2*std.pow(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可以开始训练模型啦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x1bbc0563910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device difinition\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "x_dim = 28\n",
    "h_dim = 100\n",
    "z_dim = 16\n",
    "n_layers = 1\n",
    "n_epochs = 25\n",
    "clip = 10\n",
    "learning_rate = 1e-3\n",
    "batch_size = 8 # 128\n",
    "seed = 128\n",
    "print_every = 1000 # batches\n",
    "save_every = 10    # epochs\n",
    "\n",
    "# Manual seed\n",
    "torch.manual_seed(seed)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d429b0e0bb541c189f605a9b8ce643e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f66fa0d3e24aaeaa6820dc5c623ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c725f15e7f43e8b4f469b445c05f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2a0cfa9e494018ad56b59cc4ad11eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True,\n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, \n",
    "        transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = VRNN(x_dim, h_dim, z_dim, n_layers)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "sample_data = next(iter(train_loader))\n",
    "sample_data[0].shape # torch.Size([8, 1, 28, 28])\n",
    "sample_data[1].shape # torch.Size([8])\n",
    "\n",
    "(data, _) = sample_data\n",
    "data = data.to(device)\n",
    "data = data.squeeze().transpose(0,1) # torch.Size([28, 8, 28])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "\n",
    "        #transforming data\n",
    "        data = data.to(device)\n",
    "        data = data.squeeze().transpose(0, 1) # (seq, batch, elem)\n",
    "        data = (data - data.min()) / (data.max() - data.min())\n",
    "        \n",
    "        #forward + backward + optimize\n",
    "        optimizer.zero_grad()\n",
    "        kld_loss, nll_loss, _, _ = model(data)\n",
    "        loss = kld_loss + nll_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #grad norm clipping, only in pytorch version >= 1.10\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        #printing\n",
    "        if batch_idx % print_every == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t KLD Loss: {:.6f} \\t NLL Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * batch_size, batch_size * (len(train_loader.dataset)//batch_size),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                kld_loss / batch_size,\n",
    "                nll_loss / batch_size))\n",
    "            \n",
    "            sample = model.sample(torch.tensor(28, device=device))\n",
    "            plt.imshow(sample.to(torch.device('cpu')).numpy())\n",
    "            plt.pause(1e-6)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "\n",
    "def test(epoch):\n",
    "    \"\"\"uses test data to evaluate \n",
    "    likelihood of the model\"\"\"\n",
    "\n",
    "    mean_kld_loss, mean_nll_loss = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):                                            \n",
    "\n",
    "            data = data.to(device)\n",
    "            data = data.squeeze().transpose(0, 1)\n",
    "            data = (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "            kld_loss, nll_loss, _, _ = model(data)\n",
    "            mean_kld_loss += kld_loss.item()\n",
    "            mean_nll_loss += nll_loss.item()\n",
    "\n",
    "    mean_kld_loss /= len(test_loader.dataset)\n",
    "    mean_nll_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('====> Test set loss: KLD Loss = {:.4f}, NLL Loss = {:.4f} '.format(\n",
    "        mean_kld_loss, mean_nll_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\t KLD Loss: 7.729406 \t NLL Loss: 544.679443\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\学习\\生成模型\\VRNN\\VRNN.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m#training + testing\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     test(epoch)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#saving model\u001b[39;00m\n",
      "\u001b[1;32md:\\学习\\生成模型\\VRNN\\VRNN.ipynb Cell 8\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#forward + backward + optimize\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m kld_loss, nll_loss, _, _ \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m kld_loss \u001b[39m+\u001b[39m nll_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Anaconda\\condafiles\\envs\\torch1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\学习\\生成模型\\VRNN\\VRNN.ipynb Cell 8\u001b[0m in \u001b[0;36mVRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m _, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(torch\u001b[39m.\u001b[39mcat([phi_x_t, phi_z_t],\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), h)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m \u001b[39m# Computing the loss\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=134'>135</a>\u001b[0m \u001b[39m# KL 散度计算的是 Encoder 采样出来的 z 和 先验的 z （也就是从h里面得到的z）\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=135'>136</a>\u001b[0m kld_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_kld_gauss(enc_mean_t, enc_std_t, prior_mean_t, prior_std_t)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=136'>137</a>\u001b[0m \u001b[39m#nll_loss += self._nll_gauss(dec_mean_t, dec_std_t, x[t])\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=137'>138</a>\u001b[0m nll_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nll_bernoulli(dec_mean_t, x[t])\n",
      "\u001b[1;32md:\\学习\\生成模型\\VRNN\\VRNN.ipynb Cell 8\u001b[0m in \u001b[0;36mVRNN._kld_gauss\u001b[1;34m(self, mean_1, std_1, mean_2, std_2)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_kld_gauss\u001b[39m(\u001b[39mself\u001b[39m, mean_1, std_1, mean_2, std_2):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     \u001b[39m\"\"\"Using std to compute KLD\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     kld_element \u001b[39m=\u001b[39m  (\u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mlog(std_2 \u001b[39m+\u001b[39;49m EPS) \u001b[39m-\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mlog(std_1 \u001b[39m+\u001b[39;49m EPS) \u001b[39m+\u001b[39m \n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m         (std_1\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m) \u001b[39m+\u001b[39m (mean_1 \u001b[39m-\u001b[39m mean_2)\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)) \u001b[39m/\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m         std_2\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/%E5%AD%A6%E4%B9%A0/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/VRNN/VRNN.ipynb#X11sZmlsZQ%3D%3D?line=199'>200</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\t\u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39msum(kld_element)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    #training + testing\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "\n",
    "    #saving model\n",
    "    if epoch % save_every == 1:\n",
    "        fn = 'saves/vrnn_state_dict_'+str(epoch)+'.pth'\n",
    "        torch.save(model.state_dict(), fn)\n",
    "        print('Saved model to '+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hyperparameters\n",
    "x_dim = 28\n",
    "h_dim = 100\n",
    "z_dim = 16\n",
    "n_layers =  1\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "state_dict = torch.load('saves/vrnn_state_dict_41.pth')\n",
    "model = VRNN(x_dim, h_dim, z_dim, n_layers)\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(device)\n",
    "\n",
    "sample = model.sample(28*6)\n",
    "plt.imshow(sample.cpu().numpy(), cmap='gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('torch1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "644c6c69c3445bff11595a7384ce7a4ac998fc7544c89bb4625760e004825899"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
